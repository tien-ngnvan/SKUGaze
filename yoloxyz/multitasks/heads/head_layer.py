
import torch
import torch.nn as nn
import torch.nn.functional as F
from backbones.yolov7.models.yolo import IDetect

from multitasks.models.yolov7.common import ImplicitA, ImplicitM, DWConv, Conv


class HeadLayers:
    def __init__(self, list):
        self.list = list
        
    def get_name(self, x):
        return x.__class__.__name__
    
    def check(self, x):
        return True if self.get_name(x) in self.list else False
    
class IKeypoint(nn.Module):
    stride = None  # strides computed during build
    export = False  # onnx export

    def __init__(self, nc=80, anchors=(), nkpt=5, ch=(), inplace=True, dw_conv_kpt=False):  # detection layer
        super(IKeypoint, self).__init__()

        self.nc = nc  # number of classes
        self.nkpt = nkpt
        self.dw_conv_kpt = dw_conv_kpt
        self.no_det=(nc + 5)  # number of outputs per anchor for box and class
        self.no_kpt = 3*self.nkpt ## number of outputs per anchor for keypoints
        self.no = self.no_det + self.no_kpt
        self.nl = len(anchors)  # number of detection layers
        self.na = len(anchors[0]) // 2  # number of anchors
        self.grid = [torch.zeros(1)] * self.nl  # init grid
        self.flip_test = False
        a = torch.tensor(anchors).float().view(self.nl, -1, 2)
        self.register_buffer('anchors', a)  # shape(nl,na,2)
        self.register_buffer('anchor_grid', a.clone().view(self.nl, 1, -1, 1, 1, 2))  # shape(nl,1,na,1,1,2)
        temp = 256
        for i in range(1, len(ch)):
            ch[i] = ch[i] + temp
            temp = ch[i]
        self.m = nn.ModuleList(nn.Conv2d(x, self.no_det * self.na, 1) for x in ch[1:])  # output conv 

        self.ia = nn.ModuleList(ImplicitA(x) for x in ch[1:])
        self.im = nn.ModuleList(ImplicitM(self.no_det * self.na) for _ in ch[1:])
        self.pIdetect = nn.Conv2d(self.na * self.nl * self.no_det, 256, 1)
        if self.nkpt is not None:
            if self.dw_conv_kpt: #keypoint head is slightly more complex
                self.m_kpt = nn.ModuleList(
                            nn.Sequential(DWConv(x, x, k=3), Conv(x,x),
                                          DWConv(x, x, k=3), Conv(x, x),
                                          DWConv(x, x, k=3), Conv(x,x),
                                          DWConv(x, x, k=3), Conv(x, x),
                                          DWConv(x, x, k=3), Conv(x, x),
                                          DWConv(x, x, k=3), nn.Conv2d(x, self.no_kpt * self.na, 1)) for x in ch[1:])
            else: #keypoint head is a single convolution
                self.m_kpt = nn.ModuleList(nn.Conv2d(x, self.no_kpt * self.na, 1) for x in ch[1:])

        self.inplace = inplace  # use in-place ops (e.g. slice assignment)
    def forward(self, x):
        # x = x.copy()  # for profiling
        z = []  # inference output
        x_new = []
        self.training |= self.export
        if self.training:
            x_idetect = x[0]
        else:
            x_idetect = x[0][1]
        x_idetect = x_idetect[::-1] #(bs, 3, nx, ny, 6)
        temp = F.interpolate(x_idetect[0], scale_factor=(2, 2, 1), mode='trilinear', align_corners=False)
        for i in range(1, len(x_idetect)):
            concat = torch.cat((temp, x_idetect[i]), axis = 1)
            temp = F.interpolate(concat, scale_factor=(2, 2, 1), mode='trilinear', align_corners=False)
        bs, nf, ny, nx, nc_  = concat.shape 
        concat = concat.view(bs, nf * nc_, ny, nx) #(bs, na*nl*nc, nx, ny) 
        concat = self.pIdetect(concat) # (bs, 256, nx, ny)

        temp = concat
        for i in range(1, len(x)):
            concat = torch.cat((temp, x[i]), 1) 
            x_new.append(concat)
            temp = F.interpolate(concat, scale_factor=(0.5, 0.5), mode='bilinear', align_corners=False)
        #[(bs, 512, 160, 160), (bs, 1024, 80, 80), (bs, 1792, 40, 40), (bs, 2816, 20, 20)]
        if self.export:
            for i in range(self.nl):
                if self.nkpt is None or self.nkpt==0:
                    x_new[i] = self.im[i](self.m[i](self.ia[i](x_new[i])))  # conv
                else :
                    x_new[i] = torch.cat((self.im[i](self.m[i](self.ia[i](x_new[i]))), self.m_kpt[i](x_new[i])), axis=1)
                #bs, _, ny, nx = x[i].shape  # x(bs,255,20,20) to x(bs,3,20,20,85)
                #x[i] = x[i].view(bs, self.na, self.no, ny, nx).permute(0, 1, 3, 4, 2).contiguous()
            return x_new

        for i in range(self.nl):
            if self.nkpt is None or self.nkpt==0:
                x_new[i] = self.im[i](self.m[i](self.ia[i](x_new[i])))  # conv
            else :
                x_new[i] = torch.cat((self.im[i](self.m[i](self.ia[i](x_new[i]))), self.m_kpt[i](x_new[i])), axis=1)

            bs, _, ny, nx = x_new[i].shape  # x(bs,255,20,20) to x(bs,3,20,20,85)

            x_new[i] = x_new[i].view(bs, self.na, self.no, ny, nx).permute(0, 1, 3, 4, 2).contiguous()
            
            x_det = x_new[i][..., :self.no_det].clone().detach()
            x_kpt = x_new[i][..., self.no_det:].clone().detach() # torch.Size([3, 3, 80, 80, 17])

            if not self.training:  # inference must use result
                if self.grid[i].shape[2:4] != x_new[i].shape[2:4]:
                    self.grid[i] = self._make_grid(nx, ny).to(x_new[i].device)
                kpt_grid_x = self.grid[i][..., 0:1]
                kpt_grid_y = self.grid[i][..., 1:2]

                if self.nkpt == 0:
                    y = x_new[i].sigmoid()
                else:
                    y = x_det.sigmoid()

                if self.inplace:
                    xy = (y[..., 0:2] * 2. - 0.5 + self.grid[i]) * self.stride[i]  # xy
                    wh = (y[..., 2:4] * 2) ** 2 * self.anchor_grid[i].view(1, self.na, 1, 1, 2) # wh
                    if self.nkpt != 0:
                        x_kpt[..., 1::3] = (x_kpt[..., 1::3] * 2. - 0.5 + kpt_grid_y.repeat(1,1,1,1,self.nkpt)) * self.stride[i]  # xy
                        x_kpt[..., 0::3] = (x_kpt[..., 0::3] * 2. - 0.5 + kpt_grid_x.repeat(1,1,1,1,self.nkpt)) * self.stride[i]  # xy
                        x_kpt[..., 2::3] = x_kpt[..., 2::3].sigmoid()
                    y = torch.cat((xy, wh, y[..., 4:], x_kpt), dim = -1)

                else:  # for YOLOv5 on AWS Inferentia https://github.com/ultralytics/yolov5/pull/2953
                    xy = (y[..., 0:2] * 2. - 0.5 + self.grid[i]) * self.stride[i]  # xy
                    wh = (y[..., 2:4] * 2) ** 2 * self.anchor_grid[i]  # wh
                    if self.nkpt != 0:
                        y[..., 6:] = (y[..., 6:] * 2. - 0.5 + self.grid[i].repeat((1,1,1,1,self.nkpt))) * self.stride[i]  # xy
                    y = torch.cat((xy, wh, y[..., 4:]), -1)

                z.append(y.view(bs, -1, self.no))

        return x_new if self.training else (torch.cat(z, 1), x_new)

    @staticmethod
    def _make_grid(nx=20, ny=20):
        yv, xv = torch.meshgrid([torch.arange(ny), torch.arange(nx)])
        return torch.stack((xv, yv), 2).view((1, 1, ny, nx, 2)).float()
# class IKeypoint(nn.Module):
#     stride = None  # strides computed during build
#     export = False  # onnx export

#     def __init__(self, nc=80, anchors=(), nkpt=5, ch=(), inplace=True, dw_conv_kpt=False):  # detection layer
#         super(IKeypoint, self).__init__()

#         self.nc = nc  # number of classes
#         self.nkpt = nkpt
#         self.dw_conv_kpt = dw_conv_kpt
#         self.no_det=(nc + 5)  # number of outputs per anchor for box and class
#         self.no_kpt = 3*self.nkpt ## number of outputs per anchor for keypoints
#         self.no = self.no_det+self.no_kpt
#         self.nl = len(anchors)  # number of detection layers
#         self.na = len(anchors[0]) // 2  # number of anchors
#         self.grid = [torch.zeros(1)] * self.nl  # init grid
#         self.flip_test = False
#         a = torch.tensor(anchors).float().view(self.nl, -1, 2)
#         self.register_buffer('anchors', a)  # shape(nl,na,2)
#         self.register_buffer('anchor_grid', a.clone().view(self.nl, 1, -1, 1, 1, 2))  # shape(nl,1,na,1,1,2)
#         self.m = nn.ModuleList(nn.Conv2d(x, self.no_det * self.na, 1) for x in ch)  # output conv

#         self.ia = nn.ModuleList(ImplicitA(x) for x in ch)
#         self.im = nn.ModuleList(ImplicitM(self.no_det * self.na) for _ in ch)

#         if self.nkpt is not None:
#             if self.dw_conv_kpt: #keypoint head is slightly more complex
#                 self.m_kpt = nn.ModuleList(
#                             nn.Sequential(DWConv(x, x, k=3), Conv(x,x),
#                                           DWConv(x, x, k=3), Conv(x, x),
#                                           DWConv(x, x, k=3), Conv(x,x),
#                                           DWConv(x, x, k=3), Conv(x, x),
#                                           DWConv(x, x, k=3), Conv(x, x),
#                                           DWConv(x, x, k=3), nn.Conv2d(x, self.no_kpt * self.na, 1)) for x in ch)
#             else: #keypoint head is a single convolution
#                 self.m_kpt = nn.ModuleList(nn.Conv2d(x, self.no_kpt * self.na, 1) for x in ch)

#         self.inplace = inplace  # use in-place ops (e.g. slice assignment)

#     def forward(self, x):
#         # x = x.copy()  # for profiling
#         z = []  # inference output
#         self.training |= self.export
#         if self.export:
#             for i in range(self.nl):
#                 if self.nkpt is None or self.nkpt==0:
#                     x[i] = self.im[i](self.m[i](self.ia[i](x[i])))  # conv
#                 else :
#                     x[i] = torch.cat((self.im[i](self.m[i](self.ia[i](x[i]))), self.m_kpt[i](x[i])), axis=1)
#                 #bs, _, ny, nx = x[i].shape  # x(bs,255,20,20) to x(bs,3,20,20,85)
#                 #x[i] = x[i].view(bs, self.na, self.no, ny, nx).permute(0, 1, 3, 4, 2).contiguous()
#             return x

#         for i in range(self.nl):
#             if self.nkpt is None or self.nkpt==0:
#                 x[i] = self.im[i](self.m[i](self.ia[i](x[i])))  # conv
#             else :
#                 x[i] = torch.cat((self.im[i](self.m[i](self.ia[i](x[i]))), self.m_kpt[i](x[i])), axis=1)

#             bs, _, ny, nx = x[i].shape  # x(bs,255,20,20) to x(bs,3,20,20,85)

#             x[i] = x[i].view(bs, self.na, self.no, ny, nx).permute(0, 1, 3, 4, 2).contiguous()
            
#             x_det = x[i][..., :self.no_det].clone().detach()
#             x_kpt = x[i][..., self.no_det:].clone().detach() # torch.Size([3, 3, 80, 80, 17])

#             if not self.training:  # inference 
#                 if self.grid[i].shape[2:4] != x[i].shape[2:4]:
#                     self.grid[i] = self._make_grid(nx, ny).to(x[i].device)
#                 kpt_grid_x = self.grid[i][..., 0:1]
#                 kpt_grid_y = self.grid[i][..., 1:2]

#                 if self.nkpt == 0:
#                     y = x[i].sigmoid()
#                 else:
#                     y = x_det.sigmoid()

#                 if self.inplace:
#                     xy = (y[..., 0:2] * 2. - 0.5 + self.grid[i]) * self.stride[i]  # xy
#                     wh = (y[..., 2:4] * 2) ** 2 * self.anchor_grid[i].view(1, self.na, 1, 1, 2) # wh
#                     if self.nkpt != 0:
#                         x_kpt[..., 1::3] = (x_kpt[..., 1::3] * 2. - 0.5 + kpt_grid_y.repeat(1,1,1,1,self.nkpt)) * self.stride[i]  # xy
#                         x_kpt[..., 0::3] = (x_kpt[..., 0::3] * 2. - 0.5 + kpt_grid_x.repeat(1,1,1,1,self.nkpt)) * self.stride[i]  # xy
#                         x_kpt[..., 2::3] = x_kpt[..., 2::3].sigmoid()
#                     y = torch.cat((xy, wh, y[..., 4:], x_kpt), dim = -1)

#                 else:  # for YOLOv5 on AWS Inferentia https://github.com/ultralytics/yolov5/pull/2953
#                     xy = (y[..., 0:2] * 2. - 0.5 + self.grid[i]) * self.stride[i]  # xy
#                     wh = (y[..., 2:4] * 2) ** 2 * self.anchor_grid[i]  # wh
#                     if self.nkpt != 0:
#                         y[..., 6:] = (y[..., 6:] * 2. - 0.5 + self.grid[i].repeat((1,1,1,1,self.nkpt))) * self.stride[i]  # xy
#                     y = torch.cat((xy, wh, y[..., 4:]), -1)

#                 z.append(y.view(bs, -1, self.no))

#         return x if self.training else (torch.cat(z, 1), x)

#     @staticmethod
#     def _make_grid(nx=20, ny=20):
#         yv, xv = torch.meshgrid([torch.arange(ny), torch.arange(nx)])
#         return torch.stack((xv, yv), 2).view((1, 1, ny, nx, 2)).float()
    
    
class IDetectBody(IDetect):
    def __init__(self, nc=80, anchors=(), nkpt=None, ch=(), inplace=True, dw_conv_kpt=False): # detection layer
        super(IDetectBody, self).__init__(nc, anchors, ch)
        self.nkpt= nkpt
        self.inplace = inplace
        self.dw_conv_kpt = dw_conv_kpt
        
    def fuse(self):
        print("IDetectBody.fuse")
        # fuse ImplicitA and Convolution
        for i in range(len(self.m)):
            c1,c2,_,_ = self.m[i].weight.shape
            c1_,c2_, _,_ = self.ia[i].implicit.shape
            self.m[i].bias += torch.matmul(self.m[i].weight.reshape(c1,c2),self.ia[i].implicit.reshape(c2_,c1_)).squeeze(1)

        # fuse ImplicitM and Convolution
        for i in range(len(self.m)):
            c1,c2, _,_ = self.im[i].implicit.shape
            self.m[i].bias *= self.im[i].implicit.reshape(c2)
            self.m[i].weight *= self.im[i].implicit.transpose(0,1)
            

class IDetectHead(IDetect):
    def __init__(self, nc=80, anchors=(), nkpt=None, ch=(), inplace=True, dw_conv_kpt=False): # detection layer
        super(IDetectHead, self).__init__(nc, anchors, ch)
        self.nkpt= nkpt
        self.inplace = inplace
        self.dw_conv_kpt = dw_conv_kpt
        
    def fuse(self):
        print("IDetectBody.fuse")
        # fuse ImplicitA and Convolution
        for i in range(len(self.m)):
            c1,c2,_,_ = self.m[i].weight.shape
            c1_,c2_, _,_ = self.ia[i].implicit.shape
            self.m[i].bias += torch.matmul(self.m[i].weight.reshape(c1,c2),self.ia[i].implicit.reshape(c2_,c1_)).squeeze(1)

        # fuse ImplicitM and Convolution
        for i in range(len(self.m)):
            c1,c2, _,_ = self.im[i].implicit.shape
            self.m[i].bias *= self.im[i].implicit.reshape(c2)
            self.m[i].weight *= self.im[i].implicit.transpose(0,1)